"""
llm_api.py ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –∫–ª—ñ—î–Ω—Ç –¥–ª—è LLM API.
"""

import requests
from .config import (
    LLM_API_KEY,
    LLM_BASE_URL,
    LLM_MODEL,
    LLM_TEMPERATURE,
    LLM_MAX_TOKENS,
    LLM_TOP_P,
)


class LLMAPI:
    """–ö–ª—ñ—î–Ω—Ç –¥–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ LLM."""

    BASE_URL = LLM_BASE_URL

    def __init__(self):
        self.api_key = LLM_API_KEY
        self.model = LLM_MODEL
        self.temperature = LLM_TEMPERATURE
        self.max_tokens = LLM_MAX_TOKENS

        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "LLM_TOP_P": str(LLM_TOP_P),
        }

    def generate(self, messages: list[dict]) -> str:
        """
        –ü—Ä–∏–π–º–∞—î –ø–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫ messages (system/user/assistant)
        —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.

        """

          # üîç –î–µ–±–∞–≥: –¥–∏–≤–∏–º–æ—Å—å, —â–æ —Ä–µ–∞–ª—å–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –≤ LLM
        print("\n================= MESSAGES, –Ø–ö–Ü –ô–î–£–¢–¨ –£ LLM =================")
        try:
            # –æ–∫—Ä–µ–º–æ –ø–æ–∫–∞–∂–µ–º–æ –≤—Å—ñ system-–ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
            system_msgs = [m for m in messages if m.get("role") == "system"]
            print("---- SYSTEM MESSAGES ----")
            for idx, m in enumerate(system_msgs, start=1):
                print(f"[SYSTEM #{idx}]")
                print(m.get("content", "")[:500], "...\n")

            # –∞ —Ç–∞–∫–æ–∂ –≤–µ—Å—å payload –∫—Ä–∞—Å–∏–≤–æ (–º–æ–∂–µ –±—É—Ç–∏ –¥–æ–≤–≥–∏–º)
            pretty = json.dumps(messages, ensure_ascii=False, indent=2)
            print("---- FULL MESSAGES JSON (–æ–±—Ä—ñ–∑–∞–π –æ—á–∏–º–∞, —è–∫—â–æ –¥–æ–≤–≥–æ) ----")
            print(pretty)
        except Exception as exc:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∫—Ä–∞—Å–∏–≤–æ –≤–∏–≤–µ—Å—Ç–∏ messages: {exc}")
            print(messages)
        print("============================================================\n")

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            # LLM_TOP_P –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É headers —ñ —Ç—ñ–ª—ñ; —Ä–µ—à—Ç–∞ –ø–µ–Ω–∞–ª—å—Ç—ñ –ø–æ–∫–∏ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ
            "top_p": LLM_TOP_P,
        }

        print("üåê –ù–∞–¥—Å–∏–ª–∞—é –∑–∞–ø–∏—Ç —É LLM...")

        resp = requests.post(
            self.BASE_URL,
            headers=self.headers,
            json=payload,
            timeout=30,
        )

        if resp.status_code != 200:
            raise RuntimeError(f"‚ùå –ü–æ–º–∏–ª–∫–∞ LLM API: {resp.text}")

        data = resp.json()
        answer = data["choices"][0]["message"]["content"]
        print("‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ LLM –æ—Ç—Ä–∏–º–∞–Ω–æ.")
        return answer
