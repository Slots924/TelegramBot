"""
llm_api.py ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –∫–ª—ñ—î–Ω—Ç –¥–ª—è LLM API.
"""

import requests
from .config import (
    LLM_API_KEY,
    LLM_BASE_URL,
    LLM_MODEL,
    LLM_TEMPERATURE,
    LLM_MAX_TOKENS,
    LLM_TOP_P,
)


class LLMAPI:
    """–ö–ª—ñ—î–Ω—Ç –¥–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ LLM."""

    BASE_URL = LLM_BASE_URL

    def __init__(self):
        self.api_key = LLM_API_KEY
        self.model = LLM_MODEL
        self.temperature = LLM_TEMPERATURE
        self.max_tokens = LLM_MAX_TOKENS

        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "LLM_TOP_P": str(LLM_TOP_P),
        }

    def generate(self, messages: list[dict]) -> str:
        """
        –ü—Ä–∏–π–º–∞—î –ø–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫ messages (system/user/assistant)
        —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
        """

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            # LLM_TOP_P –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É headers —ñ —Ç—ñ–ª—ñ; —Ä–µ—à—Ç–∞ –ø–µ–Ω–∞–ª—å—Ç—ñ –ø–æ–∫–∏ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ
            "top_p": LLM_TOP_P,
        }

        print("üåê –ù–∞–¥—Å–∏–ª–∞—é –∑–∞–ø–∏—Ç —É LLM...")

        resp = requests.post(
            self.BASE_URL,
            headers=self.headers,
            json=payload,
            timeout=30,
        )

        if resp.status_code != 200:
            raise RuntimeError(f"‚ùå –ü–æ–º–∏–ª–∫–∞ LLM API: {resp.text}")

        data = resp.json()
        answer = data["choices"][0]["message"]["content"]
        print("‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ LLM –æ—Ç—Ä–∏–º–∞–Ω–æ.")
        return answer
